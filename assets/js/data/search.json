[ { "title": "Nutanix AHV - Network LLDP/CDP Information", "url": "/posts/Nutanix-AHV-Network-LLDP-CDP-Information/", "categories": "Nutanix", "tags": "ahv, networking", "date": "2020-12-03 17:00:00 -0600", "snippet": "Nutanix AHV is able to receive Link Layer Discovery Protocol (LLDP) and Cisco Discovery Protocol (CDP) from the upstream switches they are connected to.To receive LLDP/CDP information can run lldpctl from an AHV host. If you’d like to pull the information from all nodes in a cluster, SSH into a Nutanix CVM and run:hostssh lldpctlThis is a great way to see details on what switch ports your Nutanix nodes are connected to.You can also get the switch port details on the Network page from Nutanix Prism. It can give a nice visual of the physical network connections.By default the Nutanix AHV hypervisor is set to receive-only for LLDP/CDP info from the network. You can enable the virtual switch in AHV to transmit (TX) LLDP/CDP info. This allows the network administrator to see the AHV hosts on the network by running show lldp neighbor or show cdp neighbor or similar on the upstream swtich.This setting is configured from the command line on a Nutanix CVM. To get the current setting run:ncli cluster get-hypervisor-lldp-configTo enable LLDP transmit on a cluster run:ncli cluster edit-hypervisor-lldp-params enable-lldp-tx=trueI’m a big fan of leveraging LLDP/CDP info instead of going into datacenters and manually tracing cables. Hopefully this can help you when troubleshooting or validating physical network connectivity." }, { "title": "How to upgrade Splunk on CentOS7", "url": "/posts/How-to-upgrade-Splunk-on-CentOS7/", "categories": "Splunk", "tags": "centos", "date": "2020-10-11 18:00:00 -0500", "snippet": "I needed to upgrade a small Splunk server from 7.1.2 to 7.3.2 on a CentOS7. Overall the process was fairly straightforward, below were my steps. When in doubt, follow the official documentation from Splunk.Download the new version of SplunkGrab the .tgz file for Splunk Enterprise – https://www.splunk.com/en_us/download/splunk-enterprise.html#tabs/linux In this case the file name was: splunk-7.3.2-c60db69f8e32-Linux-x86_64.tgzStop the Splunk ServerSSH into the splunk server and run:/opt/splunk/bin/splunk stopSnapshot the Splunk serverIf your splunk server is virtualized, it never hurts to grab a snapshot before you do the upgrade.Upgrade SplunkAssuming Splunk is installed to /opt/splunk, run the following command to upgrade splunk:tar xvzf splunk-7.3.2-c60db69f8e32-Linux-x86_64.tgz -C /optRun splunk after the upgradeThis will start Splunk after the upgrade and accept the license:/opt/splunk/bin/splunk start --accept-licenseAlso set Splunk to auto start if the server is rebooted:/opt/splunk/bin/splunk enable boot-startDelete your snapshotIf the upgrade appears to have been successful, don’t forget to delete your snapshot." }, { "title": "Determine if Hyper-Threading is Enabled on a Nutanix AHV Host", "url": "/posts/Determine-if-Hyper-Threading-is-Enabled-on-Nutanix-AHV/", "categories": "Nutanix", "tags": "ahv", "date": "2020-10-01 18:00:00 -0500", "snippet": "I had a customer ask if Hyper-Threading (HT) was enabled on their Nutanix AHV host. As of AOS 5.18 (Oct 2020) the HT status is not visible in Prism. You only see the actual cores on the host.To get the HT status, you have to jump into the CLI. SSH into a Nutanix CVM and run:hostssh \"lscpu | egrep '^Thread|^Core|^Socket|^CPU\\('\"The hostssh macro will run the following command on all hypervisor hosts in the Nutanix cluster. Very helpful!If Thread(s) per core is set to 2, then HT is enabled." }, { "title": "Nutanix Network Bandwidth Test", "url": "/posts/Nutanix-Network-Bandwidth-Test/", "categories": "Nutanix", "tags": "ahv, cvm, iperf, networking", "date": "2019-01-26 17:00:00 -0600", "snippet": "Here’s a quick script to check the network bandwidth/throughput on a Nutanix cluster. SSH into a Nutanix CVM and run:allssh ./diagnostics/diagnostics.py run_iperfThis will loop through each CVM and test network performance to the other CVM’s in the cluster. It leverages the diagnostics.py script that is builtin to each CVM.Use caution when executing this script as iPerf is designed to push the network bandwidth to the max. I typically only use this when setting up a new cluster to validate network performance or if troubleshooting a network performance issue.In the example below, you can see we are getting close to line-rate speeds between CVM’s on a 10Gbps network.login as: nutanixNutanix Controller VMnutanix@NTNX-CLUSTER password:Last login: Wed Nov 11 09:41:00 2018 from 10.20.30.108nutanix@NTNX-8675309-A-CVM:10.20.30.103:~$ allssh ./diagnostics/diagnostics.py run_iperf================== 10.20.30.101 =================Running Iperf Test between CVMsbandwidth between 10.20.30.101 and 10.20.30.102 is: 9.44 Gbitsbandwidth between 10.20.30.101 and 10.20.30.103 is: 9.41 Gbitsbandwidth between 10.20.30.101 and 10.20.30.104 is: 9.49 Gbitsbandwidth between 10.20.30.101 and 10.20.30.105 is: 9.34 Gbitsbandwidth between 10.20.30.101 and 10.20.30.106 is: 9.44 Gbitsbandwidth between 10.20.30.101 and 10.20.30.107 is: 9.46 Gbitsbandwidth between 10.20.30.101 and 10.20.30.108 is: 9.45 Gbits================== 10.20.30.102 =================Running Iperf Test between CVMsbandwidth between 10.20.30.102 and 10.20.30.101 is: 9.45 Gbitsbandwidth between 10.20.30.102 and 10.20.30.103 is: 9.47 Gbitsbandwidth between 10.20.30.102 and 10.20.30.104 is: 9.47 Gbitsbandwidth between 10.20.30.102 and 10.20.30.105 is: 9.45 Gbitsbandwidth between 10.20.30.102 and 10.20.30.106 is: 9.42 Gbitsbandwidth between 10.20.30.102 and 10.20.30.107 is: 9.46 Gbitsbandwidth between 10.20.30.102 and 10.20.30.108 is: 9.4 Gbits================== 10.20.30.103 =================Running Iperf Test between CVMsbandwidth between 10.20.30.103 and 10.20.30.101 is: 9.43 Gbitsbandwidth between 10.20.30.103 and 10.20.30.102 is: 9.45 Gbitsbandwidth between 10.20.30.103 and 10.20.30.104 is: 9.4 Gbitsbandwidth between 10.20.30.103 and 10.20.30.105 is: 9.44 Gbitsbandwidth between 10.20.30.103 and 10.20.30.106 is: 9.42 Gbitsbandwidth between 10.20.30.103 and 10.20.30.107 is: 9.48 Gbitsbandwidth between 10.20.30.103 and 10.20.30.108 is: 9.43 Gbits================== 10.20.30.104 =================Running Iperf Test between CVMsbandwidth between 10.20.30.104 and 10.20.30.101 is: 9.46 Gbitsbandwidth between 10.20.30.104 and 10.20.30.102 is: 9.49 Gbitsbandwidth between 10.20.30.104 and 10.20.30.103 is: 9.43 Gbitsbandwidth between 10.20.30.104 and 10.20.30.105 is: 9.46 Gbitsbandwidth between 10.20.30.104 and 10.20.30.106 is: 9.45 Gbitsbandwidth between 10.20.30.104 and 10.20.30.107 is: 9.43 Gbitsbandwidth between 10.20.30.104 and 10.20.30.108 is: 9.48 Gbits================== 10.20.30.105 =================Running Iperf Test between CVMsbandwidth between 10.20.30.105 and 10.20.30.101 is: 9.47 Gbitsbandwidth between 10.20.30.105 and 10.20.30.102 is: 9.48 Gbitsbandwidth between 10.20.30.105 and 10.20.30.103 is: 9.43 Gbitsbandwidth between 10.20.30.105 and 10.20.30.104 is: 9.48 Gbitsbandwidth between 10.20.30.105 and 10.20.30.106 is: 9.46 Gbitsbandwidth between 10.20.30.105 and 10.20.30.107 is: 9.36 Gbitsbandwidth between 10.20.30.105 and 10.20.30.108 is: 9.46 Gbits================== 10.20.30.106 =================Running Iperf Test between CVMsbandwidth between 10.20.30.106 and 10.20.30.101 is: 9.45 Gbitsbandwidth between 10.20.30.106 and 10.20.30.102 is: 9.44 Gbitsbandwidth between 10.20.30.106 and 10.20.30.103 is: 9.43 Gbitsbandwidth between 10.20.30.106 and 10.20.30.104 is: 9.48 Gbitsbandwidth between 10.20.30.106 and 10.20.30.105 is: 9.43 Gbitsbandwidth between 10.20.30.106 and 10.20.30.107 is: 9.44 Gbitsbandwidth between 10.20.30.106 and 10.20.30.108 is: 9.47 Gbits================== 10.20.30.107 =================Running Iperf Test between CVMsbandwidth between 10.20.30.107 and 10.20.30.101 is: 9.43 Gbitsbandwidth between 10.20.30.107 and 10.20.30.102 is: 9.41 Gbitsbandwidth between 10.20.30.107 and 10.20.30.103 is: 9.45 Gbitsbandwidth between 10.20.30.107 and 10.20.30.104 is: 9.41 Gbitsbandwidth between 10.20.30.107 and 10.20.30.105 is: 9.48 Gbitsbandwidth between 10.20.30.107 and 10.20.30.106 is: 9.46 Gbitsbandwidth between 10.20.30.107 and 10.20.30.108 is: 9.42 Gbits================== 10.20.30.108 =================Running Iperf Test between CVMsbandwidth between 10.20.30.108 and 10.20.30.101 is: 9.44 Gbitsbandwidth between 10.20.30.108 and 10.20.30.102 is: 9.46 Gbitsbandwidth between 10.20.30.108 and 10.20.30.103 is: 9.43 Gbitsbandwidth between 10.20.30.108 and 10.20.30.104 is: 9.4 Gbitsbandwidth between 10.20.30.108 and 10.20.30.105 is: 9.44 Gbitsbandwidth between 10.20.30.108 and 10.20.30.106 is: 9.47 Gbitsbandwidth between 10.20.30.108 and 10.20.30.107 is: 9.43 Gbitsnutanix@NTNX-8675309-A-CVM:10.20.30.103:~$" }, { "title": "Test Network Ports on Nutanix CVMs", "url": "/posts/Test-Network-Ports-on-Nutanix-CVMs/", "categories": "Nutanix", "tags": "ahv, cvm, networking", "date": "2018-12-12 17:00:00 -0600", "snippet": "Every now and then you’ll need to test connectivity on Nutanix CVM’s. For example, if you are setting up replication on Nutanix you’ll need TCP port 2020 open between the clusters.In Windows I would test this using telnet but since the Nutanix CVM’s are based on linux we need to use the netcat (nc) command.For example, if I wanted to test TCP port 2020 between clusters, I would SSH into a CVM and run:nc -v -w 2 10.20.30.40 -z 2020Where 10.20.30.40 is the remote IP I want to test connectivity to.Below is an example of testing TCP 2009 and 2020 to a remote cluster. You can see that port 2009 is not open as the connection times out, while 2020 is open." } ]
